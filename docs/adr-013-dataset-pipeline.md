# ADR-013: 学習データセットの構造化と自動前処理パイプライン

## 1. ステータス
**提案中 (Proposed)**

## 2. コンテキスト
TTSモデル（XTTS, GPT-SoVITS等）の学習品質は、データセットの質と一貫性に直接依存する。現状、生データから学習用フォーマットへの変換、ノイズ除去、ラベリング（文字起こし）の工程がモデルごとに個別に行われるリスクがある。これを共通化・構造化するための指針が必要である。

## 3. 決定事項

### 3.1 データセットのディレクトリ構造
`SOURCE/dataset/` 配下を以下のように構造化し、バージョン管理を行う。

```text
SOURCE/dataset/
├── raw/                # 未加工の音声ファイル
├── processed/          # クリーニング済みの音声 (WAV, 22050Hz/44100Hz)
├── transcripts/        # 文字起こしデータ (json, txt, lab)
└── metadata/           # 話者、感情、年代などのメタデータ
```

### 3.2 自動前処理パイプライン
以下の工程を Docker コンテナ内のスクリプト (`src/setup-common-env.py` 等の拡張) で自動化する。

1.  **Loudness Normalization**: すべての音声を -20 LUFS に統一。
2.  **Denoising**: AIベースのノイズ除去（Demucs または DeepFilterNet）の適用。
3.  **Slicing**: 無音部分での自動分割（1〜10秒程度）。
4.  **ASR (Automatic Speech Recognition)**: Whisper (large-v3) による高精度な文字起こし。
5.  **Validation**: 音声とテキストの一致度チェック。

### 3.3 データセットのバージョン管理
学習に使用したデータセットのサブセットを `v1`, `v2` などのタグで管理し、どのデータがどのモデル成果物に寄与したかを追跡可能にする。

## 4. 決定による影響

### メリット
- **モデル間の一貫性**: XTTSで学習したデータをそのまま GPT-SoVITS の学習に流用可能。
- **再現性**: どの前処理を施したデータで学習したかが明確になる。
- **品質向上**: Whisper v3 等の最新モデルによる自動ラベリングで、手動修正のコストを削減。

### デメリット
- **ストレージ容量**: 中間ファイル（processed）の生成により、ディスク消費量が増大する。
- **計算コスト**: 学習前の前処理（特にノイズ除去とASR）に時間を要する。

## 5. 補足
本ADRは、[adr-008-learning-plan.md](adr-008-learning-plan.md) における「特定の語彙での発音の乱れ」を、前処理段階で検知・修正するための技術的基盤となる。
