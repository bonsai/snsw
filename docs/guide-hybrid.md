# XTTS-v2 × RVC ハイブリッド構成ガイド

志ん生さんの声を再現するために、**「話し方（XTTS-v2）」**と**「声質（RVC）」**を分担させる最強の構成案です。

## 1. 役割分担

| モデル | 担当する要素 | LoRA/学習の目的 |
| :--- | :--- | :--- |
| **XTTS-v2** | 独特の「間」、抑揚、江戸弁のイントネーション | 志ん生さん特有の語り口（リズム）を覚えさせる |
| **RVC** | 声の枯れ具合、質感、倍音成分 | 生成された音声を「志ん生さんの喉」を通したような音にする |

## 2. XTTS-v2 への LoRA 適用ポイント

XTTS-v2 の GPT モデル部分に対して LoRA を適用します。

- **ターゲットモジュール**: `attn.q_proj`, `attn.v_proj` などの Attention 層を狙うのが一般的です。
- **Rank (r)**: 8〜16 程度から開始し、話し方の再現度が足りない場合は上げてください。
- **データセット**: 志ん生さんのクリアな語り（30分〜2時間程度）と、正確な文字起こしが必要です。

## 3. RVC との連携フロー

1.  **XTTS 推論**: 
    - LoRA 適用済みの XTTS で音声を生成します。
    - この時点では「話し方は似ているが、声質が少しクリアすぎる」状態を目指します。
2.  **RVC 変換**:
    - XTTS の出力を RVC モデル（志ん生さんの声で学習済み）に入力します。
    - RVC の `index` ファイルを適切に使用することで、より本人に近い質感になります。

## 4. 実装のヒント

添付の `xtts_rvc_hybrid_example.py` は、これらのモデルをプログラムからどう扱うかの骨組みを示しています。

- **学習時**: XTTS-v2 のファインチューニングには `coqui-ai/TTS` の `Trainer` クラスを使用するのが最も安定します。
- **推論時**: 生成した WAV ファイルを一時保存し、それを RVC の CLI や API に渡すパイプラインを組むのが簡単です。

---
この構成により、単一モデルでは到達できない「志ん生さんらしさ」を追求することが可能になります。
