五代目古今亭志ん生の声を再現するためには、**Pythonが動作するPC環境**、**高性能なGPU（グラフィックボード）**、そして**良質な音声データ**の3つが不可欠です。

2026年時点では、かつて主流だったTortoise TTSはメンテナンスが止まっており、**Coqui XTTS-v2**や**RVC**（Retrieval-based Voice Conversion）を組み合わせる手法が最も現実的で高品質な結果を得られます。

具体的な機材と環境の詳細は以下の通りです。

### 1. 必要なハードウェア（PCスペック）
AIの学習（ファインチューニング）を行う場合、ビデオメモリ（VRAM）の容量が重要になります。

*   **GPU (VRAM):**
    *   **8〜12GB:** 初心者向けのWebUI（AllTalk TTSなど）で試行可能。
    *   **16〜24GB以上:** 本格的な学習を行う場合の推奨。NVIDIAの**A100**などがあれば非常に快適です。
*   **ストレージ:** 音声データや学習済みモデルを保存するため、余裕のあるSSD容量が必要です。

### 2. 音声データの収集・編集環境
志ん生特有の「枯れた声」や「間」を再現するには、データの質が成功の9割を決めます。

*   **音源収集ツール:** **yt-dlp**（YouTubeなどの動画から高品質なWAV形式で音声を抽出するため）。
*   **音声編集ソフト:** **Audacity**や**pydub**（Pythonライブラリ）。
    *   長い落語から、拍手、観客の笑い声、無音部分をカットして「志ん生の声だけ」を抽出するために使用します。
*   **文字起こしツール:** **Whisper (large-v3)**。
    *   学習には音声と一致する正確なテキスト（トランスクリプト）が必要なため、自動生成した後に手修正を行います。

### 3. ソフトウェア・モデル環境
以下のスタックを構築することが推奨されます。

*   **基本環境:** **Python + PyTorch** 環境。
*   **メインモデル (TTS):** **Coqui XTTS-v2**。
    *   日本語品質が高く、数秒〜数分の参照音声でクローニングが可能です。
*   **変換モデル (VC):** **RVC (Retrieval-based Voice Conversion)**。
    *   感情のニュアンスや「間」をより強調したい場合、XTTSで生成した音声をRVCでさらに変換するハイブリッド方式が最強のクオリティを生みます。
*   **学習UI:** 初心者には **AllTalk TTS** や **xtts-finetune-webui** が推奨されます。

### 4. 再現のためのデータ目安
*   **手軽に試す（ゼロショット）:** 30秒〜数分のクリアな参照音声。
*   **本格的に再現する（ファインチューニング）:** 最低1時間、理想的には**3〜10時間**のノイズのない音声データが必要です。

志ん生師匠の再現において、特に「この演目のこの時期の声（絶頂期か、晩年の枯れた声か）」といったこだわりはありますか？それによって準備すべき音源の選び方が変わってきます。
# ADR 002: v2ディレクトリ構成の再編とモデル管理方針

## ステータス
提案中 (Proposed)

## コンテキスト
現在、`v2` ディレクトリには以下の要素がフラットに混在しており、役割分担が不明確になっている。

1.  **分析ツール**: `inspector.py`, `diagnostician.py` など、音声品質を評価するユーティリティ。
2.  **実験的ノートブック**: `qwen4 copy.ipynb`, `rora.ipynb` など、新しいモデル（Qwen-TTS, PEFT/LoRA）を検証するためのコード。
3.  **ドキュメント**: `docs/` 以下に旧来の方針（XTTS/RVC）が記載されているが、実態（Qwen/PEFT）と乖離し始めている。

このままでは、どのファイルが「再利用可能なコンポーネント」で、どれが「使い捨ての実験コード」なのか判別しづらい。また、Qwen-TTSなどの新しいモデルを導入する際、どこに実装を置くべきかの指針がない。

## 決定事項
`v2` ディレクトリの構造を以下のように再編し、役割を明確化する。

### 1. ディレクトリ構成の定義

```
v2/
├── models/         # 再利用可能なモデル定義、ラッパー、学習スクリプト
│   ├── qwen/       # Qwen-TTS関連
│   └── peft/       # PEFT/LoRA学習関連
├── experiments/    # 検証用ノートブック、一時的な実験コード
│   ├── qwen_poc/   # Qwen-TTSの検証
│   └── lora_poc/   # LoRA学習の検証
├── tools/          # 汎用的な分析・診断・前処理ツール
│   ├── inspector.py
│   └── diagnostician.py
└── docs/
    └── adr/        # Architecture Decision Records
```

### 2. モデル採用方針の更新
既存のドキュメントでは **Coqui XTTS-v2 + RVC** を推奨していたが、v2フェーズでは以下のモデルも検証・採用対象として正式に扱う。

*   **Qwen-TTS**: 高品質なゼロショット音声合成の有力候補として検証を行う。
*   **PEFT (LoRA)**: 大規模モデルの効率的なファインチューニング手法として、実装を `models/peft/` に集約する。

### 3. 実装の移行
*   既存の `*.ipynb` ファイルは、原則として `experiments/` ディレクトリへ移動する。
*   ノートブック内で定義されているクラスや関数（例: 推論ロジック）は、安定した段階で `models/` 内のPythonモジュールへ切り出す。

## 結果 (Consequences)

### メリット
*   **関心の分離**: 実験コードと本番用ライブラリコードが分離され、メンテナンス性が向上する。
*   **可読性**: 新規参画者がプロジェクト構造を把握しやすくなる。
*   **拡張性**: 新しいモデル（例: F5-TTSなど）を試す際、`experiments/` で試し、`models/` に昇格させるフローが確立する。

### デメリット
*   **パスの修正**: ノートブックを移動するため、ファイルパス（`../text.json` など）の修正が必要になる。
*   **移行コスト**: 既存ファイルの移動とリファクタリングの手間が発生する。

## 補足
本ADRの作成に伴い、既存の `v2/docs/ADR.md` は `v2/docs/adr/0001-legacy-architecture.md` (仮) として扱い、今後は `v2/docs/adr/` 下でナンバリング管理を行う。
# ファイルオーケストレーター設計: 出世魚ループ

## 1. エージェント進化（出世魚）の定義
ループの各段階を以下の名称で管理します。

| 段階 (Iteration) | 出世魚名 | 役割/状態 |
| :--- | :--- | :--- |
| **v1** | **WAKASHI (ワカシ)** | 初期生成・ベースライン |
| **v2** | **INADA (イナダ)** | 診断に基づく第一次改善 |
| **v3** | **WARASA (ワラサ)** | 高度なパラメータ調整・第二次改善 |
| **Final** | **BURI (ブリ)** | 最終完成形（最新コード） |

## 2. ファイル命名規則 (outputフォルダ)
全ての成果物を `/home/ubuntu/output/` (Colabでは `/content/drive/MyDrive/VC/output/`) に集約します。

- **WAV**: `{rank}_audio.wav`
- **Code**: `{rank}_code.py`
- **Report**: `{rank}_report.json`
- **Summary**: `shusseuo_ledger.md` (出世魚台帳)

## 3. オーケストレーターの責務
1.  現在のランク（魚名）を決定
2.  TTSコードを実行し、`{rank}_audio.wav` を保存
3.  診断を実行し、`{rank}_report.json` を保存
4.  改善案を練り、次回のランク用の `{rank}_code.py` を生成
5.  最後に `BURI_final_code.py` を出力
